{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA4N9Wj0ciUnW8xSGYmb7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fxs2596/ECommerce/blob/main/ECommerce_ML_Train_Test_store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Complete Google Colab Notebook Code for Churn Prediction ---\n",
        "\n",
        "# --- Step 0: Import Necessary Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy for np.inf, np.nan\n",
        "# Removed: import plotly.express as px # Removed visualization library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import datetime # Needed for date calculations\n",
        "\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "\n",
        "# --- Step 1: Load the Prepared ML Dataset ---\n",
        "\n",
        "# Load the prepared ML dataset CSV\n",
        "# Make sure 'customer_churn_ml_dataset.csv' is uploaded to your Colab environment\n",
        "file_name = 'customer_churn_ml_dataset.csv'\n",
        "\n",
        "try:\n",
        "    df_ml = pd.read_csv(file_name)\n",
        "    print(f\"\\nML dataset '{file_name}' loaded successfully! Shape: {df_ml.shape}\")\n",
        "    print(\"Columns:\", df_ml.columns.tolist())\n",
        "    print(\"\\nFirst 5 rows of the ML dataset:\")\n",
        "    print(df_ml.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: File '{file_name}' not found. Please upload the CSV file to your Colab environment.\")\n",
        "    # In Colab, you might need to upload it using the file browser on the left\n",
        "    # or mount Google Drive if the file is there.\n",
        "    # Example for mounting Google Drive:\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # file_name = '/content/drive/My Drive/path/to/your/customer_churn_ml_dataset.csv'\n",
        "    exit() # Stop execution if file not found\n",
        "\n",
        "# --- Step 2: Data Preparation and Feature Engineering (if not already in CSV) ---\n",
        "# Note: Our CSV already contains the engineered features (RFM, Tenure, AOV)\n",
        "# and the churn label, so this step is primarily for verification or adding new features.\n",
        "# If you were starting from raw orders data, this section would be more extensive.\n",
        "\n",
        "print(\"\\nVerifying/Preparing data structure...\")\n",
        "# Ensure essential columns are present\n",
        "required_cols = ['Recency', 'Frequency', 'Monetary', 'DaysSinceFirstOrder', 'AverageOrderValue', 'churn']\n",
        "if not all(col in df_ml.columns for col in required_cols):\n",
        "    print(\"\\nError: Essential columns for ML are missing in the CSV.\")\n",
        "    print(\"Missing columns:\", [col for col in required_cols if col not in df_ml.columns])\n",
        "    exit()\n",
        "\n",
        "# Ensure numerical columns are numeric types\n",
        "numerical_features = ['Recency', 'Frequency', 'Monetary', 'DaysSinceFirstOrder', 'AverageOrderValue'] # Define numerical features list\n",
        "for col in numerical_features + ['churn']: # Include churn in the loop\n",
        "     if col in df_ml.columns:\n",
        "         df_ml[col] = pd.to_numeric(df_ml[col], errors='coerce') # Coerce errors will turn non-numeric into NaN\n",
        "\n",
        "# Handle potential NaN values created by coercion (e.g., fill with mean, median, or 0)\n",
        "# For simplicity, fill NaNs in features with 0 (consider more sophisticated imputation for real projects)\n",
        "df_ml[numerical_features] = df_ml[numerical_features].fillna(0)\n",
        "\n",
        "\n",
        "# --- Step 3: Separate Features (X) and Target (y) ---\n",
        "\n",
        "print(\"\\nSeparating features (X) and target (y)...\")\n",
        "# The target variable is 'churn'\n",
        "target = 'churn'\n",
        "\n",
        "# Features are the engineered numerical features\n",
        "ml_features = ['Recency', 'Frequency', 'Monetary', 'DaysSinceFirstOrder', 'AverageOrderValue']\n",
        "X = df_ml[ml_features]\n",
        "y = df_ml[target]\n",
        "\n",
        "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "print(\"\\nFeatures used:\", ml_features)\n",
        "\n",
        "\n",
        "# --- Step 4: Split Data into Training and Testing Sets ---\n",
        "# We split the df_ml dataset itself for model training and evaluation\n",
        "\n",
        "print(\"\\nSplitting data into training (80%) and testing (20%) sets...\")\n",
        "# Using stratify=y is important for classification to maintain the same churn distribution in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTrain set shape (X_train, y_train): {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test set shape (X_test, y_test): {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 5: Select and Train the Logistic Regression Model ---\n",
        "\n",
        "print(\"\\nInitializing and training Logistic Regression model...\")\n",
        "# Initialize the Logistic Regression model\n",
        "# random_state for reproducibility\n",
        "# solver='liblinear' is often good for small datasets and binary classification\n",
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Train the model on the training data\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nLogistic Regression model training complete.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Make Predictions and Evaluate the Model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test set...\")\n",
        "# Predict class labels (0 or 1)\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Predict probabilities of the positive class (churn=1)\n",
        "y_prob = log_reg_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nEvaluating the model...\")\n",
        "# Calculate standard classification metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob) # ROC AUC is a good metric for imbalanced data\n",
        "\n",
        "print(\"\\n--- Logistic Regression Model Evaluation on Test Set ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\nModel training and evaluation complete.\")\n",
        "\n",
        "# You now have a trained Logistic Regression model (log_reg_model)\n",
        "# and its performance metrics on the test set.\n",
        "# This streamlined notebook focuses on the core ML pipeline steps.\n",
        "# The next step would be to use this trained model to predict churn probability\n",
        "# for your *current* customers (if you had a separate dataset of current customers\n",
        "# without a churn label) and potentially integrate those predictions into your dashboard.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsInP4rCsskA",
        "outputId": "b94f312f-4601-4fd7-8c03-13fef0155ea3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n",
            "\n",
            "ML dataset 'customer_churn_ml_dataset.csv' loaded successfully! Shape: (432, 8)\n",
            "Columns: ['customer_id', 'name', 'Recency', 'Frequency', 'Monetary', 'DaysSinceFirstOrder', 'AverageOrderValue', 'churn']\n",
            "\n",
            "First 5 rows of the ML dataset:\n",
            "   customer_id              name  Recency  Frequency  Monetary  \\\n",
            "0            1      Norma Fisher      543          1    227.14   \n",
            "1            2   Steven Robinson      102          3    555.44   \n",
            "2            3  Theodore Mcgrath      628          1    308.34   \n",
            "3            4    Brian Hamilton       72          1     43.37   \n",
            "4            5       Thomas Moon      165          1    384.37   \n",
            "\n",
            "   DaysSinceFirstOrder  AverageOrderValue  churn  \n",
            "0                  543         227.140000      1  \n",
            "1                  668         185.146667      0  \n",
            "2                  628         308.340000      1  \n",
            "3                   72          43.370000      0  \n",
            "4                  165         384.370000      0  \n",
            "\n",
            "Verifying/Preparing data structure...\n",
            "\n",
            "Separating features (X) and target (y)...\n",
            "\n",
            "Features (X) shape: (432, 5)\n",
            "Target (y) shape: (432,)\n",
            "\n",
            "Features used: ['Recency', 'Frequency', 'Monetary', 'DaysSinceFirstOrder', 'AverageOrderValue']\n",
            "\n",
            "Splitting data into training (80%) and testing (20%) sets...\n",
            "\n",
            "Train set shape (X_train, y_train): (345, 5), (345,)\n",
            "Test set shape (X_test, y_test): (87, 5), (87,)\n",
            "\n",
            "Initializing and training Logistic Regression model...\n",
            "\n",
            "Logistic Regression model training complete.\n",
            "\n",
            "Making predictions on the test set...\n",
            "\n",
            "Evaluating the model...\n",
            "\n",
            "--- Logistic Regression Model Evaluation on Test Set ---\n",
            "Accuracy: 0.9655\n",
            "Precision: 0.9375\n",
            "Recall: 1.0000\n",
            "F1-Score: 0.9677\n",
            "ROC AUC: 0.9947\n",
            "\n",
            "Model training and evaluation complete.\n"
          ]
        }
      ]
    }
  ]
}